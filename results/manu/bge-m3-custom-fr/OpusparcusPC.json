{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 22.200496912002563,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.10",
  "scores": {
    "test.full": [
      {
        "cos_sim": {
          "accuracy": 0.8160762942779292,
          "accuracy_threshold": 0.6991962973745462,
          "ap": 0.9333973385767004,
          "f1": 0.8721590909090909,
          "f1_threshold": 0.6991962973745462,
          "precision": 0.8334841628959276,
          "recall": 0.9145978152929494
        },
        "dot": {
          "accuracy": 0.8153950953678474,
          "accuracy_threshold": 0.6995299232696937,
          "ap": 0.9333777967323726,
          "f1": 0.871939736346516,
          "f1_threshold": 0.695892800910201,
          "precision": 0.8290062667860341,
          "recall": 0.9195630585898709
        },
        "euclidean": {
          "accuracy": 0.8160762942779292,
          "accuracy_threshold": 0.7754933883161386,
          "ap": 0.9333928799726494,
          "f1": 0.8721590909090909,
          "f1_threshold": 0.7754933883161386,
          "precision": 0.8334841628959276,
          "recall": 0.9145978152929494
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9333973385767004,
        "manhattan": {
          "accuracy": 0.8160762942779292,
          "accuracy_threshold": 19.581595331430435,
          "ap": 0.9327319047100688,
          "f1": 0.8727440999537251,
          "f1_threshold": 20.09659779071808,
          "precision": 0.817157712305026,
          "recall": 0.9364448857994042
        },
        "max": {
          "accuracy": 0.8160762942779292,
          "ap": 0.9333973385767004,
          "f1": 0.8727440999537251
        }
      }
    ],
    "validation.full": [
      {
        "cos_sim": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 0.6919957080623012,
          "ap": 0.9447003083949628,
          "f1": 0.8816901408450705,
          "f1_threshold": 0.6894114471565155,
          "precision": 0.8287731685789939,
          "recall": 0.9418254764292878
        },
        "dot": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 0.6921116122845774,
          "ap": 0.944701727209905,
          "f1": 0.8816901408450705,
          "f1_threshold": 0.6894707637552528,
          "precision": 0.8287731685789939,
          "recall": 0.9418254764292878
        },
        "euclidean": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 0.7849297579416925,
          "ap": 0.9446974891315663,
          "f1": 0.8816901408450705,
          "f1_threshold": 0.7881817983280568,
          "precision": 0.8287731685789939,
          "recall": 0.9418254764292878
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.944701727209905,
        "manhattan": {
          "accuracy": 0.8183760683760684,
          "accuracy_threshold": 19.650558561086655,
          "ap": 0.9445920954595324,
          "f1": 0.8798882681564245,
          "f1_threshold": 20.167656302452087,
          "precision": 0.8210251954821894,
          "recall": 0.9478435305917753
        },
        "max": {
          "accuracy": 0.8205128205128205,
          "ap": 0.944701727209905,
          "f1": 0.8816901408450705
        }
      }
    ]
  },
  "task_name": "OpusparcusPC"
}