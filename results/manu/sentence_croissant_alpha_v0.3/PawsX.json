{
  "dataset_revision": "8a04d940a42cd40658986fdd8e3da561533a3646",
  "evaluation_time": 34.88610529899597,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.10",
  "scores": {
    "test": [
      {
        "cos_sim": {
          "accuracy": 0.6385,
          "accuracy_threshold": 0.9508013558065057,
          "ap": 0.6538959709008848,
          "f1": 0.6387570133793699,
          "f1_threshold": 0.879734296578379,
          "precision": 0.5233380480905233,
          "recall": 0.8194905869324474
        },
        "dot": {
          "accuracy": 0.6385,
          "accuracy_threshold": 0.9508013659173142,
          "ap": 0.6544238415520034,
          "f1": 0.6387570133793699,
          "f1_threshold": 0.8797343239996112,
          "precision": 0.5233380480905233,
          "recall": 0.8194905869324474
        },
        "euclidean": {
          "accuracy": 0.6385,
          "accuracy_threshold": 0.31368342111771647,
          "ap": 0.6538959709008848,
          "f1": 0.6387570133793699,
          "f1_threshold": 0.4904400178209911,
          "precision": 0.5233380480905233,
          "recall": 0.8194905869324474
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.6544238415520034,
        "manhattan": {
          "accuracy": 0.6375,
          "accuracy_threshold": 9.935688418442169,
          "ap": 0.6543682552093091,
          "f1": 0.637968144640551,
          "f1_threshold": 17.36533879641098,
          "precision": 0.5218309859154929,
          "recall": 0.8205980066445183
        },
        "max": {
          "accuracy": 0.6385,
          "ap": 0.6544238415520034,
          "f1": 0.6387570133793699
        }
      }
    ]
  },
  "task_name": "PawsX"
}