{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 21.93565273284912,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.10",
  "scores": {
    "test.full": [
      {
        "cos_sim": {
          "accuracy": 0.8283378746594006,
          "accuracy_threshold": 0.38418928610130465,
          "ap": 0.9336964271182351,
          "f1": 0.8799249530956847,
          "f1_threshold": 0.3394741765439364,
          "precision": 0.8337777777777777,
          "recall": 0.9314796425024826
        },
        "dot": {
          "accuracy": 0.8283378746594006,
          "accuracy_threshold": 0.38418928601908164,
          "ap": 0.9336964271182351,
          "f1": 0.8799249530956847,
          "f1_threshold": 0.3394741686218503,
          "precision": 0.8337777777777777,
          "recall": 0.9314796425024826
        },
        "euclidean": {
          "accuracy": 0.8283378746594006,
          "accuracy_threshold": 1.1097843566563512,
          "ap": 0.9336964271182351,
          "f1": 0.8799249530956847,
          "f1_threshold": 1.149369854915542,
          "precision": 0.8337777777777777,
          "recall": 0.9314796425024826
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9343137300140691,
        "manhattan": {
          "accuracy": 0.8269754768392371,
          "accuracy_threshold": 38.79594994553838,
          "ap": 0.9343137300140691,
          "f1": 0.8793264733395698,
          "f1_threshold": 40.672316694788606,
          "precision": 0.8311229000884174,
          "recall": 0.9334657398212513
        },
        "max": {
          "accuracy": 0.8283378746594006,
          "ap": 0.9343137300140691,
          "f1": 0.8799249530956847
        }
      }
    ],
    "validation.full": [
      {
        "cos_sim": {
          "accuracy": 0.8311965811965812,
          "accuracy_threshold": 0.3511822616914345,
          "ap": 0.9407666861671445,
          "f1": 0.8862217954872781,
          "f1_threshold": 0.3511822616914345,
          "precision": 0.8499079189686924,
          "recall": 0.925777331995988
        },
        "dot": {
          "accuracy": 0.8311965811965812,
          "accuracy_threshold": 0.35118226707923583,
          "ap": 0.9407666861671445,
          "f1": 0.8862217954872781,
          "f1_threshold": 0.35118226707923583,
          "precision": 0.8499079189686924,
          "recall": 0.925777331995988
        },
        "euclidean": {
          "accuracy": 0.8311965811965812,
          "accuracy_threshold": 1.1391380002360798,
          "ap": 0.9407674140916502,
          "f1": 0.8862217954872781,
          "f1_threshold": 1.1391380002360798,
          "precision": 0.8499079189686924,
          "recall": 0.925777331995988
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9412488037708715,
        "manhattan": {
          "accuracy": 0.8326210826210826,
          "accuracy_threshold": 39.95324218483455,
          "ap": 0.9412488037708715,
          "f1": 0.8867469879518073,
          "f1_threshold": 39.95324218483455,
          "precision": 0.8534322820037106,
          "recall": 0.9227683049147443
        },
        "max": {
          "accuracy": 0.8326210826210826,
          "ap": 0.9412488037708715,
          "f1": 0.8867469879518073
        }
      }
    ]
  },
  "task_name": "OpusparcusPC"
}