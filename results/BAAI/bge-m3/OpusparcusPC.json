{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 31.111865758895874,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.10",
  "scores": {
    "test.full": [
      {
        "cos_sim": {
          "accuracy": 0.8119891008174387,
          "accuracy_threshold": 0.6995917229634472,
          "ap": 0.9288387194483139,
          "f1": 0.867231638418079,
          "f1_threshold": 0.6640944848784405,
          "precision": 0.8245299910474485,
          "recall": 0.9145978152929494
        },
        "dot": {
          "accuracy": 0.8119891008174387,
          "accuracy_threshold": 0.6995946307480949,
          "ap": 0.9288198516967311,
          "f1": 0.867231638418079,
          "f1_threshold": 0.6641015487550135,
          "precision": 0.8245299910474485,
          "recall": 0.9145978152929494
        },
        "euclidean": {
          "accuracy": 0.8126702997275205,
          "accuracy_threshold": 0.7804618666861409,
          "ap": 0.9288541715265348,
          "f1": 0.867231638418079,
          "f1_threshold": 0.8195461658919294,
          "precision": 0.8245299910474485,
          "recall": 0.9145978152929494
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9288541715265348,
        "manhattan": {
          "accuracy": 0.8126702997275205,
          "accuracy_threshold": 19.755515664815903,
          "ap": 0.9281377367028953,
          "f1": 0.8678890456041373,
          "f1_threshold": 20.702426075935364,
          "precision": 0.8241071428571428,
          "recall": 0.916583912611718
        },
        "max": {
          "accuracy": 0.8126702997275205,
          "ap": 0.9288541715265348,
          "f1": 0.8678890456041373
        }
      }
    ],
    "validation.full": [
      {
        "cos_sim": {
          "accuracy": 0.8148148148148148,
          "accuracy_threshold": 0.6732858257696122,
          "ap": 0.9457226797107524,
          "f1": 0.8761271950640722,
          "f1_threshold": 0.6688050393388816,
          "precision": 0.8315315315315316,
          "recall": 0.925777331995988
        },
        "dot": {
          "accuracy": 0.8148148148148148,
          "accuracy_threshold": 0.6732201374200883,
          "ap": 0.9457289365299991,
          "f1": 0.8761271950640722,
          "f1_threshold": 0.6688547801579006,
          "precision": 0.8315315315315316,
          "recall": 0.925777331995988
        },
        "euclidean": {
          "accuracy": 0.8148148148148148,
          "accuracy_threshold": 0.8083098454816114,
          "ap": 0.9457226944237125,
          "f1": 0.8761271950640722,
          "f1_threshold": 0.8139036632588678,
          "precision": 0.8315315315315316,
          "recall": 0.925777331995988
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9457289365299991,
        "manhattan": {
          "accuracy": 0.8169515669515669,
          "accuracy_threshold": 20.33615481853485,
          "ap": 0.945664967651451,
          "f1": 0.8770923003347681,
          "f1_threshold": 20.33615481853485,
          "precision": 0.8382084095063985,
          "recall": 0.9197592778335005
        },
        "max": {
          "accuracy": 0.8169515669515669,
          "ap": 0.9457289365299991,
          "f1": 0.8770923003347681
        }
      }
    ]
  },
  "task_name": "OpusparcusPC"
}