{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 217.38782382011414,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.19",
  "scores": {
    "test.full": [
      {
        "cos_sim": {
          "accuracy": 0.8044959128065395,
          "accuracy_threshold": 0.9065432730971709,
          "ap": 0.9199728356434089,
          "f1": 0.8656995788488536,
          "f1_threshold": 0.9065432730971709,
          "precision": 0.8185840707964602,
          "recall": 0.9185700099304865
        },
        "dot": {
          "accuracy": 0.8044959128065395,
          "accuracy_threshold": 0.906543297444205,
          "ap": 0.9199728356434089,
          "f1": 0.8656995788488536,
          "f1_threshold": 0.906543297444205,
          "precision": 0.8185840707964602,
          "recall": 0.9185700099304865
        },
        "euclidean": {
          "accuracy": 0.8044959128065395,
          "accuracy_threshold": 0.4323348491778623,
          "ap": 0.9199728356434089,
          "f1": 0.8656995788488536,
          "f1_threshold": 0.4323348491778623,
          "precision": 0.8185840707964602,
          "recall": 0.9185700099304865
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9199728356434089,
        "manhattan": {
          "accuracy": 0.8044959128065395,
          "accuracy_threshold": 21.26891741289834,
          "ap": 0.9196358998786356,
          "f1": 0.8656995788488536,
          "f1_threshold": 21.26891741289834,
          "precision": 0.8185840707964602,
          "recall": 0.9185700099304865
        },
        "max": {
          "accuracy": 0.8044959128065395,
          "ap": 0.9199728356434089,
          "f1": 0.8656995788488536
        }
      }
    ],
    "validation.full": [
      {
        "cos_sim": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 0.9145601084707615,
          "ap": 0.9395017624590254,
          "f1": 0.8772426817752598,
          "f1_threshold": 0.9048778502059664,
          "precision": 0.8287243532560215,
          "recall": 0.9317953861584755
        },
        "dot": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 0.9145601607792544,
          "ap": 0.9395017624590254,
          "f1": 0.8772426817752598,
          "f1_threshold": 0.9048779003263112,
          "precision": 0.8287243532560215,
          "recall": 0.9317953861584755
        },
        "euclidean": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 0.41337607949738076,
          "ap": 0.9395017624590254,
          "f1": 0.8772426817752598,
          "f1_threshold": 0.4361699562828759,
          "precision": 0.8287243532560215,
          "recall": 0.9317953861584755
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9395017624590254,
        "manhattan": {
          "accuracy": 0.8205128205128205,
          "accuracy_threshold": 20.347758220240905,
          "ap": 0.9394816974588287,
          "f1": 0.8778877887788779,
          "f1_threshold": 21.493356750524526,
          "precision": 0.8282918149466192,
          "recall": 0.9338014042126379
        },
        "max": {
          "accuracy": 0.8205128205128205,
          "ap": 0.9395017624590254,
          "f1": 0.8778877887788779
        }
      }
    ]
  },
  "task_name": "OpusparcusPC"
}