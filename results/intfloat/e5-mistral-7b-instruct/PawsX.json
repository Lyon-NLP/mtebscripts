{
  "dataset_revision": "8a04d940a42cd40658986fdd8e3da561533a3646",
  "evaluation_time": 255.51867961883545,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.19",
  "scores": {
    "test": [
      {
        "cos_sim": {
          "accuracy": 0.612,
          "accuracy_threshold": 0.9956029654298926,
          "ap": 0.5982432095921794,
          "f1": 0.6256983240223464,
          "f1_threshold": 0.9204327670428212,
          "precision": 0.45690973992860784,
          "recall": 0.9922480620155039
        },
        "dot": {
          "accuracy": 0.612,
          "accuracy_threshold": 0.9956030552262002,
          "ap": 0.5960694716910933,
          "f1": 0.6256983240223464,
          "f1_threshold": 0.920432808563856,
          "precision": 0.45690973992860784,
          "recall": 0.9922480620155039
        },
        "euclidean": {
          "accuracy": 0.612,
          "accuracy_threshold": 0.09377670164655799,
          "ap": 0.5982432095921794,
          "f1": 0.6256983240223464,
          "f1_threshold": 0.39891622316495434,
          "precision": 0.45690973992860784,
          "recall": 0.9922480620155039
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.6008082215984939,
        "manhattan": {
          "accuracy": 0.612,
          "accuracy_threshold": 4.157702027159477,
          "ap": 0.6008082215984939,
          "f1": 0.6258741258741258,
          "f1_threshold": 18.77003094976982,
          "precision": 0.4573326520183955,
          "recall": 0.991140642303433
        },
        "max": {
          "accuracy": 0.612,
          "ap": 0.6008082215984939,
          "f1": 0.6258741258741258
        }
      }
    ]
  },
  "task_name": "PawsX"
}