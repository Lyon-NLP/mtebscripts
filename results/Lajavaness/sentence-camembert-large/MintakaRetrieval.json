{
  "dataset_revision": "efa78cc2f74bbcd21eff2261f9e13aebe40b814e",
  "evaluation_time": 10.014642238616943,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.19",
  "scores": {
    "test": [
      {
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.22464,
        "map_at_1": 0.12735,
        "map_at_10": 0.19182,
        "map_at_100": 0.19777,
        "map_at_1000": 0.19892,
        "map_at_20": 0.19506,
        "map_at_3": 0.17554,
        "map_at_5": 0.18486,
        "mrr_at_1": 0.12735,
        "mrr_at_10": 0.19182,
        "mrr_at_100": 0.19777,
        "mrr_at_1000": 0.19892,
        "mrr_at_20": 0.19506,
        "mrr_at_3": 0.17554,
        "mrr_at_5": 0.18486,
        "ndcg_at_1": 0.12735,
        "ndcg_at_10": 0.22464,
        "ndcg_at_100": 0.25779,
        "ndcg_at_1000": 0.30096,
        "ndcg_at_20": 0.23646,
        "ndcg_at_3": 0.19115,
        "ndcg_at_5": 0.2078,
        "precision_at_1": 0.12735,
        "precision_at_10": 0.03284,
        "precision_at_100": 0.00494,
        "precision_at_1000": 0.00086,
        "precision_at_20": 0.01876,
        "precision_at_3": 0.07876,
        "precision_at_5": 0.05528,
        "recall_at_1": 0.12735,
        "recall_at_10": 0.32842,
        "recall_at_100": 0.49386,
        "recall_at_1000": 0.862,
        "recall_at_20": 0.3751,
        "recall_at_3": 0.23628,
        "recall_at_5": 0.27641
      }
    ]
  },
  "task_name": "MintakaRetrieval"
}