{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 7.611276865005493,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.19",
  "scores": {
    "test.full": [
      {
        "cos_sim": {
          "accuracy": 0.8440054495912807,
          "accuracy_threshold": 0.34151358234264156,
          "ap": 0.94281163271082,
          "f1": 0.8901678657074341,
          "f1_threshold": 0.33700797810625654,
          "precision": 0.8608534322820037,
          "recall": 0.9215491559086395
        },
        "dot": {
          "accuracy": 0.8440054495912807,
          "accuracy_threshold": 0.3415135858895704,
          "ap": 0.9428131982731722,
          "f1": 0.8901678657074341,
          "f1_threshold": 0.33700799455191655,
          "precision": 0.8608534322820037,
          "recall": 0.9215491559086395
        },
        "euclidean": {
          "accuracy": 0.8440054495912807,
          "accuracy_threshold": 1.1475940640613105,
          "ap": 0.94281163271082,
          "f1": 0.8901678657074341,
          "f1_threshold": 1.1515138333900952,
          "precision": 0.8608534322820037,
          "recall": 0.9215491559086395
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9428131982731722,
        "manhattan": {
          "accuracy": 0.8440054495912807,
          "accuracy_threshold": 29.055584682655763,
          "ap": 0.9426946054730426,
          "f1": 0.889425398358281,
          "f1_threshold": 29.055584682655763,
          "precision": 0.8656015037593985,
          "recall": 0.9145978152929494
        },
        "max": {
          "accuracy": 0.8440054495912807,
          "ap": 0.9428131982731722,
          "f1": 0.8901678657074341
        }
      }
    ],
    "validation.full": [
      {
        "cos_sim": {
          "accuracy": 0.8404558404558404,
          "accuracy_threshold": 0.35953745345433563,
          "ap": 0.9533318136331852,
          "f1": 0.8903591682419659,
          "f1_threshold": 0.2881441020737603,
          "precision": 0.8418230563002681,
          "recall": 0.9448345035105316
        },
        "dot": {
          "accuracy": 0.8404558404558404,
          "accuracy_threshold": 0.3595374334491449,
          "ap": 0.9533318136331852,
          "f1": 0.8903591682419659,
          "f1_threshold": 0.28814410248013445,
          "precision": 0.8418230563002681,
          "recall": 0.9448345035105316
        },
        "euclidean": {
          "accuracy": 0.8404558404558404,
          "accuracy_threshold": 1.13177958181605,
          "ap": 0.9533318136331852,
          "f1": 0.8903591682419659,
          "f1_threshold": 1.1931939463050476,
          "precision": 0.8418230563002681,
          "recall": 0.9448345035105316
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.9533318136331852,
        "manhattan": {
          "accuracy": 0.8411680911680912,
          "accuracy_threshold": 28.670644958920548,
          "ap": 0.9532507367271332,
          "f1": 0.8904176904176904,
          "f1_threshold": 28.670644958920548,
          "precision": 0.8728323699421965,
          "recall": 0.9087261785356068
        },
        "max": {
          "accuracy": 0.8411680911680912,
          "ap": 0.9533318136331852,
          "f1": 0.8904176904176904
        }
      }
    ]
  },
  "task_name": "OpusparcusPC"
}